{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Positive / Negative Text Classifiier\n",
    "\n",
    "Before doing our subreddit analysis, we will train a model to classify whether a title is positive or negative. We will be using a dataset of 1,600,000 labelled Tweets ([download](http://kaggle.com/kazanova/sentiment140)).\n",
    "\n",
    "## Setup\n",
    "The dependancies are listed out in requirements.txt. They can be quickly installed with Pip by running the following command\n",
    "\n",
    "`python -m pip -r requirements.txt`\n",
    "\n",
    "## Summary\n",
    "We succesfully preprocessed a corpus of Twitter Tweets and used them to train a Naive-Bayes Classifier with ~71% accuracy. The model has been pickled to `bin/classifer.o` for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as wnl\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re, random, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "Some things that need to be done before we can train our model.\n",
    "\n",
    "1. Tokenization\n",
    "   All our tweets need to be tokenized to be processed. Tokenization isn't as simple as something like `str.split('.')` though. For example, \"Mr.John likes iced coffee\". If we ran this code, then the sentence would be improperly tokenized since \"Mr.John\" should not be split into 2 tokens.\n",
    "\n",
    "2. Lemmatization\n",
    "   This is the process of mapping a word to its root. It is similar to stemming. Many words have the same meaning. For instance, \"great\", \"greater\", and \"greatest\" all have the same root.\n",
    "\n",
    "3. Normalization - Stopword removal + Lowercasing\n",
    "\n",
    "   Uppercase and lowercase words hold the same value. As well, we will remove stopwords. Stopwords are commonly used words in language that are not *significant* parts of the sentence. For example, \"a\", \"are\", \"may\".\n",
    "4. Noise Removal\n",
    "\n",
    "   Remove twitter handles, hashtags, phone numbers, and special characters which can interfere which hold no text value and may interfere with our training process.\n",
    "   With our dataset, emoji's have already been removed.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a generator to get all tokens in a list of sentences\n",
    "\"\"\"\n",
    "def get_all_tokens(sentences):\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            yield token\n",
    "\n",
    "\"\"\"\n",
    "Returns the corresponding wordnet tag for a parts of speach (pos) tag\n",
    "\"\"\"\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\"\"\"\n",
    "Return dictionary for existing dataset to for model\n",
    "\"\"\"\n",
    "def sentence_to_dict(dataset):\n",
    "    for tokens in dataset:\n",
    "        yield dict([token, True] for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 799999 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  10000 non-null  int64 \n",
      " 1   text       10000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')\n",
    "df.columns = ['sentiment', 'id', 'date', 'flag', 'user', 'text']\n",
    "df.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True) # Dont need these cols\n",
    "\n",
    "positives = df.loc[df['sentiment'] == 4]\n",
    "negatives = df.loc[df['sentiment'] == 0]\n",
    "\n",
    "df = pd.concat([positives.head(5000), negatives.head(5000)])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(text, lang='english'):\n",
    "    lemmatizer = wnl()\n",
    "    stop_words = stopwords.words(lang)\n",
    "\n",
    "    result = []\n",
    "        \n",
    "    for part, tag in pos_tag(text.split()):\n",
    "        # Remove links\n",
    "        part = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', part)\n",
    "\n",
    "        # Remove handles\n",
    "        part = re.sub('(@[A-Za-z0-9_]+)', '', part)\n",
    "\n",
    "        part = part.lower().strip()\n",
    "\n",
    "        part = part.replace(':)', 'smile')\n",
    "        part = part.replace(':(', 'sad')\n",
    "        part = part.replace(':/', 'frown')\n",
    "        part = part.replace(';)', 'wink')\n",
    "        part = part.replace(':D', 'big smile')\n",
    "        part = part.replace(';D', 'big smile')\n",
    "        part = part.replace(';d', 'big smile')\n",
    "\n",
    "        # If it's a stopword we dont want to add to our token list\n",
    "        if part in stop_words:\n",
    "            continue\n",
    "\n",
    "        wordnet_pos = get_wordnet_tag(tag)\n",
    "\n",
    "        result.append(lemmatizer.lemmatize(part, wordnet_pos))\n",
    "        \n",
    "    return \" \".join(result)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Preprocessing\n",
    "\n",
    "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.youtube.com/titomi15..watch videos,comment&amp;suscribe!! &amp; follow , cheer x god bless\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "We need to format the data as a labelled featureset. This is a requirement to train the model.\n",
    "We will do a standard 80/20 train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "df = shuffle(df)\n",
    "\n",
    "# Partition to train and test\n",
    "df_train, df_test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dict\n",
    "def create_dataset(df):\n",
    "    dataset = []\n",
    "    for i, row in df.iterrows():\n",
    "        tweet = row['text']\n",
    "        sent = row['sentiment']\n",
    "        text_dict = dict([word, True] for word in word_tokenize(tweet))\n",
    "        dataset.append((text_dict, sent))\n",
    "    return dataset\n",
    "        \n",
    "d_train = create_dataset(df_train)\n",
    "d_test = create_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'almost': True, 'bedtime': True}, 0)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training our model with the Naive Bayes Classifier and using `pickle` to serialize our model so it can be reused later. It is 'naive' since it assumes that all our features are independent of each other. In our data, we only have 2 features (text, positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7046666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(d_train)\n",
    "\n",
    "print('Accuracy: {}'.format(classify.accuracy(classifier, d_test)))\n",
    "\n",
    "# Save our model so we can reuse it later!\n",
    "pickle.dump(classifier, open('bin/classifier.o', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "This is a good model for now. But we can make some improvements in the future\n",
    "\n",
    "- We can try out other model architectures such as LSTM neural network if we have good data\n",
    "- Improve preprocessing such as adding text enrichment where needed\n",
    "- Add another category, \"Neutral\"\n",
    "- Checkout the ROC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
